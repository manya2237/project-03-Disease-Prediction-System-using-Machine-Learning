{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqO3oc0j2LWE",
        "outputId": "0d63035c-bbd4-4f2b-edd7-253c11f3af9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 8763\n",
            "Number of columns: 12\n",
            "\n",
            "First few rows of the dataset:\n",
            "   Age     Sex  Cholesterol Blood Pressure  Diabetes  Family History  Smoking  \\\n",
            "0   67    Male          208         158/88         0               0        1   \n",
            "1   21    Male          389         165/93         1               1        1   \n",
            "2   21  Female          324         174/99         1               0        0   \n",
            "3   84    Male          383        163/100         1               1        1   \n",
            "4   66    Male          318          91/88         1               1        1   \n",
            "\n",
            "   Obesity  Alcohol Consumption  Physical Activity Days Per Week        BMI  \\\n",
            "0        0                    0                                0  31.251233   \n",
            "1        1                    1                                1  27.194973   \n",
            "2        0                    0                                4  28.176571   \n",
            "3        0                    1                                3  36.464704   \n",
            "4        1                    0                                1  21.809144   \n",
            "\n",
            "   Heart Attack Risk  \n",
            "0                  0  \n",
            "1                  0  \n",
            "2                  0  \n",
            "3                  0  \n",
            "4                  0  \n"
          ]
        }
      ],
      "source": [
        "from typing_extensions import dataclass_transform\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "data= pd.read_csv('dataset.csv')\n",
        "\n",
        "# Display number of rows and columns\n",
        "num_rows, num_cols = data.shape\n",
        "print(\"Number of rows:\", num_rows)\n",
        "print(\"Number of columns:\", num_cols)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"\\nFirst few rows of the dataset:\")\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "print(\"Missing values:\\n\", missing_values)\n",
        "\n",
        "# Handle missing values\n",
        "# No missing values found in this dataset, so no action needed\n",
        "\n",
        "# Handle outliers\n",
        "# We'll use z-score method to remove outliers for numerical features\n",
        "\n",
        "# Function to remove outliers using z-score method\n",
        "def remove_outliers_zscore(df, threshold=3):\n",
        "    z_scores = np.abs(stats.zscore(df.select_dtypes(include=['float64', 'int64'])))\n",
        "    filtered_entries = (z_scores < threshold).all(axis=1)\n",
        "    return df[filtered_entries]\n",
        "\n",
        "# Remove outliers\n",
        "data = remove_outliers_zscore(data)\n",
        "\n",
        "#converting into numerical data\n",
        "data['Sex'] = data['Sex'].map({'Male': 0, 'Female': 1})\n",
        "\n",
        "\n",
        "data[['Systolic', 'Diastolic']] = data['Blood Pressure'].str.split('/', expand=True).astype(int)\n",
        "\n",
        "data.drop(['Blood Pressure'], axis=1, inplace=True)\n",
        "scaler = StandardScaler()\n",
        "numerical_cols = ['Age', 'Cholesterol', 'BMI','Physical Activity Days Per Week','Systolic','Diastolic']  # Select numerical columns to normalize\n",
        "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
        "\n",
        "# Print the preprocessed data\n",
        "print(\"Preprocessed data:\\n\", data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lXnxbBF2XJn",
        "outputId": "e3c06d03-af83-41aa-f518-65d078fabdf9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values:\n",
            " Age                                0\n",
            "Sex                                0\n",
            "Cholesterol                        0\n",
            "Blood Pressure                     0\n",
            "Diabetes                           0\n",
            "Family History                     0\n",
            "Smoking                            0\n",
            "Obesity                            0\n",
            "Alcohol Consumption                0\n",
            "Physical Activity Days Per Week    0\n",
            "BMI                                0\n",
            "Heart Attack Risk                  0\n",
            "dtype: int64\n",
            "Preprocessed data:\n",
            "         Age  Sex  Cholesterol  Diabetes  Family History  Smoking  Obesity  \\\n",
            "0  0.625557    0    -0.641579         0               0        1        0   \n",
            "1 -1.539322    0     1.596895         1               1        1        1   \n",
            "2 -1.539322    1     0.793023         1               0        0        0   \n",
            "3  1.425621    0     1.522691         1               1        1        0   \n",
            "4  0.578495    0     0.718820         1               1        1        1   \n",
            "\n",
            "   Alcohol Consumption  Physical Activity Days Per Week       BMI  \\\n",
            "0                    0                        -1.528843  0.373454   \n",
            "1                    1                        -1.090738 -0.268479   \n",
            "2                    0                         0.223577 -0.113134   \n",
            "3                    1                        -0.214528  1.198524   \n",
            "4                    0                        -1.090738 -1.120826   \n",
            "\n",
            "   Heart Attack Risk  Systolic  Diastolic  \n",
            "0                  0  0.870044   0.193782  \n",
            "1                  0  1.135714   0.534480  \n",
            "2                  0  1.477290   0.943319  \n",
            "3                  0  1.059809   1.011458  \n",
            "4                  0 -1.672797   0.193782  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data.drop('Heart Attack Risk', axis=1)\n",
        "y = data['Heart Attack Risk']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the features\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "urhpvGQd2a68"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## svm\n",
        "\n",
        "\n",
        "svm_classifier = SVC()\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "svm_classifier_scores = cross_val_score(svm_classifier, X_train_scaled, y_train, cv=kf, scoring='accuracy')\n",
        "print(f'SVM Cross-Validation Scores: {svm_classifier_scores}')\n",
        "print(f'SVM Mean Accuracy: {svm_classifier_scores.mean()}')\n",
        "\n",
        "svm_param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "}\n",
        "\n",
        "# Initialize the GridSearchCV object\n",
        "svm_grid_search = GridSearchCV(SVC(), svm_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "svm_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "print(f'Best SVM Parameters: {svm_grid_search.best_params_}')\n",
        "print(f'Best SVM Score: {svm_grid_search.best_score_}')\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_svm_classifier = svm_grid_search.best_estimator_\n",
        "best_svm_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled testing data\n",
        "y_pred_svm_best = best_svm_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Print classification report for SVM with the best parameters\n",
        "print(\"Classification Report for SVM (Best Params):\")\n",
        "print(classification_report(y_test, y_pred_svm_best,zero_division=1))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYdb-ZG92dr-",
        "outputId": "5ffca92e-2480-4385-dc9b-87778c5fbcce"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Cross-Validation Scores: [0.63694722 0.64550642 0.64336662 0.65049929 0.62910128]\n",
            "SVM Mean Accuracy: 0.6410841654778888\n",
            "Best SVM Parameters: {'C': 0.1, 'kernel': 'linear'}\n",
            "Best SVM Score: 0.6417974322396576\n",
            "Classification Report for SVM (Best Params):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      1.00      0.78      1125\n",
            "           1       1.00      0.00      0.00       628\n",
            "\n",
            "    accuracy                           0.64      1753\n",
            "   macro avg       0.82      0.50      0.39      1753\n",
            "weighted avg       0.77      0.64      0.50      1753\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## decision tree\n",
        "\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "dt_classifier_scores = cross_val_score(dt_classifier, X_train_scaled, y_train, cv=kf, scoring='accuracy')\n",
        "print(f'Decision Tree Cross-Validation Scores: {dt_classifier_scores}')\n",
        "print(f'Decision Tree Mean Accuracy: {dt_classifier_scores.mean()}')\n",
        "\n",
        "dt_param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize the GridSearchCV object\n",
        "dt_grid_search = GridSearchCV(DecisionTreeClassifier(), dt_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "dt_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "print(f'Best Decision Tree Parameters: {dt_grid_search.best_params_}')\n",
        "print(f'Best Decision Tree Score: {dt_grid_search.best_score_}')\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_dt_classifier = dt_grid_search.best_estimator_\n",
        "best_dt_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled testing data\n",
        "y_pred_dt_best = best_dt_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Print classification report for Decision Tree with the best parameters\n",
        "print(\"Classification Report for Decision Tree (Best Params):\")\n",
        "print(classification_report(y_test, y_pred_dt_best))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmza2fau2gTG",
        "outputId": "fbc9130a-0be8-46a0-c95e-56665026e6ce"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Cross-Validation Scores: [0.55634807 0.5235378  0.56062767 0.53495007 0.53138374]\n",
            "Decision Tree Mean Accuracy: 0.5413694721825962\n",
            "Best Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
            "Best Decision Tree Score: 0.6279600570613411\n",
            "Classification Report for Decision Tree (Best Params):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.91      0.75      1125\n",
            "           1       0.34      0.08      0.13       628\n",
            "\n",
            "    accuracy                           0.62      1753\n",
            "   macro avg       0.49      0.50      0.44      1753\n",
            "weighted avg       0.53      0.62      0.53      1753\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "rf_classifier = RandomForestClassifier()\n",
        "rf_classifier_scores = cross_val_score(rf_classifier, X_train_scaled, y_train, cv=kf, scoring='accuracy')\n",
        "print(f'Random Forest Cross-Validation Scores: {rf_classifier_scores}')\n",
        "print(f'Random Forest Mean Accuracy: {rf_classifier_scores.mean()}')\n",
        "\n",
        "\n",
        "# Define the parameter grid for Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "}\n",
        "\n",
        "# Initialize the GridSearchCV object\n",
        "rf_grid_search = GridSearchCV(RandomForestClassifier(), rf_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "rf_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "print(f'Best Random Forest Parameters: {rf_grid_search.best_params_}')\n",
        "print(f'Best Random Forest Score: {rf_grid_search.best_score_}')\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_rf_classifier = rf_grid_search.best_estimator_\n",
        "best_rf_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled testing data\n",
        "y_pred_rf_best = best_rf_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Print classification report for Random Forest with the best parameters\n",
        "print(\"Classification Report for Random Forest (Best Params):\")\n",
        "print(classification_report(y_test, y_pred_rf_best))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwB0p12V2vry",
        "outputId": "18514b4f-bed4-4bf0-a747-eb8d5b4c2626"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Cross-Validation Scores: [0.62838802 0.62482168 0.62767475 0.64479315 0.61982882]\n",
            "Random Forest Mean Accuracy: 0.6291012838801711\n",
            "Best Random Forest Parameters: {'max_depth': 10, 'n_estimators': 200}\n",
            "Best Random Forest Score: 0.6410841654778888\n",
            "Classification Report for Random Forest (Best Params):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      1.00      0.78      1125\n",
            "           1       0.40      0.00      0.01       628\n",
            "\n",
            "    accuracy                           0.64      1753\n",
            "   macro avg       0.52      0.50      0.39      1753\n",
            "weighted avg       0.56      0.64      0.50      1753\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## logistic regressinon\n",
        "\n",
        "\n",
        "logistic_reg = LogisticRegression(max_iter=1000)\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "logistic_reg = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "logistic_reg_scores = cross_val_score(logistic_reg, X_train_scaled, y_train, cv=kf, scoring='accuracy')\n",
        "print(f'Logistic Regression Cross-Validation Scores: {logistic_reg_scores}')\n",
        "print(f'Logistic Regression Mean Accuracy: {logistic_reg_scores.mean()}')\n",
        "\n",
        "# Define the parameter grid for Logistic Regression\n",
        "logistic_param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['newton-cg', 'lbfgs', 'liblinear']\n",
        "}\n",
        "\n",
        "# Initialize the GridSearchCV object\n",
        "logistic_grid_search = GridSearchCV(LogisticRegression(max_iter=1000, class_weight='balanced'), logistic_param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the model\n",
        "logistic_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "print(f'Best Logistic Regression Parameters: {logistic_grid_search.best_params_}')\n",
        "print(f'Best Logistic Regression Score: {logistic_grid_search.best_score_}')\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_logistic_reg = logistic_grid_search.best_estimator_\n",
        "best_logistic_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled testing data\n",
        "y_pred_logistic_best = best_logistic_reg.predict(X_test_scaled)\n",
        "\n",
        "# Print classification report for logistic regression with the best parameters\n",
        "print(\"Classification Report for Logistic Regression (Best Params):\")\n",
        "print(classification_report(y_test, y_pred_logistic_best, zero_division=1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiEwxfZx3gY7",
        "outputId": "625f0f7e-62c6-4ced-ef1f-b4cba9eed7eb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Cross-Validation Scores: [0.4914408  0.50927247 0.51141227 0.50927247 0.50285307]\n",
            "Logistic Regression Mean Accuracy: 0.5048502139800285\n",
            "Best Logistic Regression Parameters: {'C': 0.1, 'solver': 'newton-cg'}\n",
            "Best Logistic Regression Score: 0.5052781740370899\n",
            "Classification Report for Logistic Regression (Best Params):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.50      0.56      1125\n",
            "           1       0.36      0.52      0.43       628\n",
            "\n",
            "    accuracy                           0.50      1753\n",
            "   macro avg       0.51      0.51      0.49      1753\n",
            "weighted avg       0.55      0.50      0.51      1753\n",
            "\n"
          ]
        }
      ]
    }
  ]
}